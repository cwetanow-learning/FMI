```
#!/bin/bash

all=$(find $1 -type f | wc -l)
uniqDuplicates=$(fdupes -r $1 | egrep '^$' | wc -l)
allDuplicates=$(fdupes -r $1 | egrep -v '^$' | wc -l)
res=$(($all-$allDuplicates+$uniqDuplicates))

echo "Count of unique files is $res"

fdupes -r $1 | egrep -v '^$' | while read file; do
	rm "$file"
done
```
Скрипта приема директорията, която искаме да проверим за уникални файлове и да изтрием повтарящите се. 

```
all=$(find $1 -type f | wc -l)
```
Този ред намира броя на всички файлове в подадената директория.

```
uniqDuplicates=$(fdupes -r $1 | egrep '^$' | wc -l)
```
Тук се намира броя на уникалните файлове, които имат дубликати

```
allDuplicates=$(fdupes -r $1 | egrep -v '^$' | wc -l)
```
Този ред открива броя на всички дублиращи се файлове

```
res=$(($all-$allDuplicates+$uniqDuplicates))

echo "Count of unique files is $res"
```
Записваме броя на уникалните файлове в променливата res и я изкарваме

```
fdupes -r $1 | egrep -v '^$' | while read file; do
	rm "$file"
done
```
С 'fdupes -r $1' изкарваме всички дублиращи се файлове.
С pipe към "egrep -v '^$'" премахва празните редове от резултата (fdupes слага по 1 празен ред след всяка група от еднакви файлове). След това обхождаме резултата в while цикъл и трием всеки файл, като слагаме променливата $file в "", за да не се приема като няколко аргумента, ако има празно място